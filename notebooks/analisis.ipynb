{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd3b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5b7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from models.transformer_lm import MiniTransformerLM\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de93dc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniTransformerLM(\n",
       "  (token_emb): Embedding(50257, 256)\n",
       "  (pos_emb): Embedding(128, 256)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=256, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vocab_model = \"gpt2\"\n",
    "model_path = \"../checkpoints/minitransformer.pt\"\n",
    "max_tokens = 128\n",
    "num_samples = 5\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(vocab_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Modelo\n",
    "model = MiniTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    max_len=max_tokens\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4754dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de generación autoregresiva\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generar_texto(prompt, max_len=100, top_k=50, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    # Agregar token por defecto si el prompt está vacío\n",
    "    if not prompt.strip():\n",
    "        prompt = tokenizer.eos_token  \n",
    "\n",
    "    ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device).long()\n",
    "    attn_mask = torch.ones_like(ids)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(ids, attention_mask=attn_mask)\n",
    "        next_logits = logits[:, -1, :] / temperature\n",
    "        top_k_probs, top_k_indices = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "        probs = torch.nn.functional.softmax(top_k_probs, dim=-1)\n",
    "        next_token = top_k_indices[0, torch.multinomial(probs, num_samples=1)]\n",
    "        next_token = next_token.view(1, 1) \n",
    "        ids = torch.cat([ids, next_token], dim=1)\n",
    "        attn_mask = torch.ones_like(ids)\n",
    "\n",
    "    return tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Métricas de diversidad\n",
    "def distinct_n(seqs, n):\n",
    "    total_ngrams = 0\n",
    "    unique_ngrams = set()\n",
    "    for text in seqs:\n",
    "        tokens = text.split()\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        ngrams = list(ngrams)\n",
    "        total_ngrams += len(ngrams)\n",
    "        unique_ngrams.update(ngrams)\n",
    "    return len(unique_ngrams) / total_ngrams if total_ngrams > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1524959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      ", to and had The was big  you. very with so the mom to you. \" a was.. had. is up were that was was in said a, she the they to the. the they The the she.. wanted him a they. time you,. to saw,.., his they, to their and said He to,. the. They. was to was a,.\" said \" a that day was and They.. so a they to and time\n",
      "------------------------------\n",
      ".., They the. he. He \" day with little she. to with He was's the They, not and. a The,. a in. so They.. it..\" and and she her to was, and.\" were of the that a and his mom and. was was you. their they day her they she \" The had. \" and. for not, , She,. to said a it wanted and to He it. that, up She a He\n",
      "------------------------------\n",
      " they little., The the and his the He and the to the's was day it time \" little to, The.. to. to she time. were happy she a.. to, was little the on he the very was they not you and, time and was \" The.'s and.. a with she. the saw Lily, their for She a \" she, you, so she the. with I and They not Lily it a., was to his,..\n",
      "------------------------------\n",
      " her  was Lily . He,. was her the!, the it,. He. He the. She...\" She said and. a The, and mom They little It, had She with is saw it was the a.. day., up it time the it He to of said,. the it \". you she She the. not. a him to said.,'s. I day. and said He her. that and very that., I mom\n",
      "------------------------------\n",
      " very. was to was the He for \" the The it their and. little not \" not \" on day He very her of,. in, said the and the mom his. and him happy.\" it,! very, he wanted.\". happy and had the was. and to on very time and. and she to. wanted day for you They, The to and the They, said saw It, the., him was were she She she. happy the a and time wanted and\n",
      "\n",
      "Métricas de diversidad:\n",
      "Distinct-1: 0.3106\n",
      "Distinct-2: 0.9116\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de generación\n",
    "num_samples = 5\n",
    "max_tokens = 100\n",
    "top_k = 50\n",
    "temperature = 1.0\n",
    "\n",
    "# Generar muestras\n",
    "resultados = []\n",
    "for _ in range(num_samples):\n",
    "    texto = generar_texto(\"\", max_len=max_tokens, top_k=top_k, temperature=temperature)\n",
    "    print(\"-\" * 30)\n",
    "    print(texto)\n",
    "    resultados.append(texto)\n",
    "\n",
    "# Calcular diversidad\n",
    "d1 = distinct_n(resultados, 1)\n",
    "d2 = distinct_n(resultados, 2)\n",
    "print(\"\\nMétricas de diversidad:\")\n",
    "print(f\"Distinct-1: {d1:.4f}\")\n",
    "print(f\"Distinct-2: {d2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
